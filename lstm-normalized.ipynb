{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (ocrhelpers.py, line 262)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3326\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-04bf89988e1d>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from ocrlib import ocrhelpers as helpers\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/tmb/exp/icdar2019-worksheets/ocrlib/ocrhelpers.py\"\u001b[0;36m, line \u001b[0;32m262\u001b[0m\n\u001b[0;31m    def set_lr(self, lr, momentum=0.9):\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import os, sys, re, glob, time, pickle, IPython, logging\n",
    "import scipy.ndimage as ndi\n",
    "from itertools import islice\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchmore import layers, flex\n",
    "import torchtrainers as tt\n",
    "from torch.utils.data import DataLoader\n",
    "from webdataset import WebDataset\n",
    "from ocrlib import ocrhelpers as helpers\n",
    "from ocrlib.ocrhelpers import *\n",
    "from ocrlib import ocrmodels as models\n",
    "RUN(\"date\"); RUN(\"hostname\"); RUN(\"whoami\"); RUN(\"nvidia-smi -L\")\n",
    "\n",
    "charset = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "def encode_str(s): \n",
    "    return [charset.find(c)+1 for c in s]\n",
    "\n",
    "def decode_str(l): \n",
    "    return \"\".join([charset[k-1] for k in l])\n",
    "\n",
    "transforms = [\n",
    "    lambda x: (torch.tensor(x).float()/255.0).unsqueeze(0),\n",
    "    lambda s: torch.tensor(encode_str(s)).long()\n",
    "]\n",
    "training = WebDataset(\"data/words-simple-training.tar\", decoder=\"l8\", \n",
    "                      extensions=\"jpg;jpeg;ppm;png txt\", transforms=transforms)\n",
    "testing = WebDataset(\"data/words-simple-test.tar\", decoder=\"l8\", \n",
    "                     extensions=\"jpg;jpeg;ppm;png txt\", transforms=transforms)\n",
    "training_dl = DataLoader(training, batch_size=5, collate_fn=helpers.collate4ocr)\n",
    "testing_dl = DataLoader(testing, batch_size=20, collate_fn=helpers.collate4ocr)\n",
    "next(iter(training_dl))[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_moments(image):\n",
    "    assert amin(image) >= 0\n",
    "    d0, d1 = image.shape\n",
    "    c0, c1 = mgrid[0:d0, 0:d1]\n",
    "    m0 = sum(c0*image) / sum(image)\n",
    "    m1 = sum(c1*image) / sum(image)\n",
    "    m00 = sum((c0-m0)*(c0-m0)*image) / sum(image)\n",
    "    m01 = sum((c0-m0)*(c1-m1)*image) / sum(image)\n",
    "    m11 = sum((c1-m1)*(c1-m1)*image) / sum(image)\n",
    "    return array([m0, m1]), array([[m00, m01], [m01, m11]])\n",
    "\n",
    "def normalize(image, height=80, r=5):\n",
    "    assert amin(image) >= 0 and amax(image) <= 1\n",
    "    h, w = image.shape\n",
    "    (y, x), ((m00, m01), (m10, m11)) = compute_moments(image > amax(image)*0.5)\n",
    "    scale = 2.0*r*(m00**.5) / height\n",
    "    delta = y - scale*(height/2.0)\n",
    "    result = ndi.affine_transform(image, scale*eye(2), \n",
    "                                  order=1,\n",
    "                                  offset=(delta, 0), \n",
    "                                  output_shape=(height, int(w/scale)))\n",
    "    assert result.shape[0] == height, result.shape\n",
    "    return result\n",
    "\n",
    "image = zeros((100, 400))\n",
    "image[10:20,10:300] = 1.0\n",
    "normalized = normalize(image)\n",
    "subplot(211); imshow(image)\n",
    "subplot(212); imshow(normalized)\n",
    "#normalize(normalized);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = [\n",
    "    lambda x: torch.tensor(normalize(x/255.0)).unsqueeze(0),\n",
    "    lambda s: torch.tensor(encode_str(s)).long()\n",
    "]\n",
    "\n",
    "training = WebDataset(\"data/words-simple-training.tar\", decoder=\"l8\", \n",
    "                      extensions=\"jpg;jpeg;ppm;png txt\", transforms=transforms)\n",
    "testing = WebDataset(\"data/words-simple-test.tar\", decoder=\"l8\", \n",
    "                     extensions=\"jpg;jpeg;ppm;png txt\", transforms=transforms)\n",
    "training_dl = DataLoader(training, batch_size=5,\n",
    "                         collate_fn=helpers.collate4ocr)\n",
    "testing_dl = DataLoader(testing, batch_size=20,\n",
    "                        collate_fn=helpers.collate4ocr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12, 8)\n",
    "for i, sample in enumerate(islice(training_dl, 0, 60, 10)):\n",
    "    subplot(6, 2, i+1)\n",
    "    imshow(sample[0][0,0].detach().numpy())\n",
    "    title(str(sample[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk '/lstm_normalized/' RS=\"\\n\\n\" ocrlib/ocrmodels.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.make(\"lstm_normalized\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = helpers.LineTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(training_dl, 10, every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(10, 10)\n",
    "for i, batch in enumerate(islice(training_dl, 0, 10)):\n",
    "    subplot(5, 2, i+1)\n",
    "    result = trainer.predict_batch(*batch[:2])\n",
    "    imshow(batch[0][0,0].detach().numpy())\n",
    "    title(decode_str(result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.errors(testing_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
